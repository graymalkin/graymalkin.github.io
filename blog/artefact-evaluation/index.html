<!DOCTYPE html>
<html lang="en">
  <title>Artefact evaluation &mdash; Simon Cooksey Blog</title>
  <meta name="viewport" content="width=device-width, user-scalable=yes">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/firacode@6.2.0/distr/fira_code.css">
  <link rel="stylesheet" type="text/css" href="/style.css"/>
  <meta name="description" content="Simon's Blog, with personal projects and some academic musings.">
  <meta charset="utf-8">
</head>
<body>
  <main>
    <h1 class="title"><span class="headline-word">Artefact</span> <span
class="headline-word">evaluation</span></h1>
    <div class="byline"><span class="headline-word">Repoducabilty</span>
<span class="headline-word">and</span> <span
class="headline-word">longevity</span> <span
class="headline-word">of</span> <span
class="headline-word">experimental</span> <span
class="headline-word">measurement</span></div>
    <div class="date">2024-04-12</div>
    <nav>
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="/blog/">Blog</a></li>
            <li><a href="/iso-papers">ISO Papers</a></li>
            <li><a href="/photography/">Photography</a></li>
        </ul>
    </nav>
<p>I believe in artefact evaluation quite wholeheartedly: papers often
cannot contain all the details to replicate a piece of science because
of space constraints, especially in conference publications; and
measurements shouldn’t just be taken on faith, it is important to
reproduce them independently. Submission of artefacts along with papers
goes a long way to fixing both concerns.</p>
<p>Sadly, I’m yet to have a positive experience on either side of the
review committee.</p>
<h2 id="as-a-submitter"><span class="headline-word">As</span> <span
class="headline-word">a</span> <span
class="headline-word">submitter…</span></h2>
<p>On the submission side it’s extremely cumbersome to do double-blind
submission, and complex research-ware issues are hard to work around.
There is a wide skill range of reviewers and wide range of hardware
access. I think the programming languages community is beginning to find
its feet here, maybe artefact evaluation is working better in other
communities.</p>
<p>At ECOOP last year they tried artefact-with-paper submission, where
the paper reviewers would get access to the artefact reviews when
writing their paper review. This is a cool idea but was very poorly
communicated to authors, so artefacts were rushed. It also necessitated
double-blind (rather than single-blind) artefact submission. This was a
huge pain for the paper we’d submitted, we had prototype hardware we
couldn’t make available for testing on (without breaking
double-blindness).</p>
<h2 id="as-a-reviewer"><span class="headline-word">As</span> <span
class="headline-word">a</span> <span
class="headline-word">reviewer…</span></h2>
<p>It seems to be consensus that artefacts should, where possible, be
made available “forever” so that they can be reproduced/reused by
scientists in the future. Zenodo is a common choice of host for such
things. At ASPLOS I have had an artefact which depends on a large
proprietary EDA tool (at a specific version), which cannot be included
with the Zenodo upload. Sadly, I think this means the artefact on Zenodo
is doomed to bit rot. As reviewers, we can recommend the artefact is
awarded either a “functional” or “reusable”. These choices for badge
names do distinguish between artefacts which work today and can be
expected to work tomorrow, verses those which cannot. We need words
which reflect that the evaluation of an artefact is a snapshot in time.
If the AEC can be sure the functionality of the artefact won’t change
over time, then it can be considered “functional/reusable
forever(ish)”.</p>
<h2 id="other-concerns-across-computer-science"><span
class="headline-word">Other</span> <span
class="headline-word">concerns</span> <span
class="headline-word">across</span> <span
class="headline-word">Computer</span> <span
class="headline-word">Science</span></h2>
<p>I understand there are similar concerns in HPC and machine learning,
where training sets are vast and hard/impossible to host “forever”, and
the hardware requirements to attempt to reproduce results are
prohibitive. This is assuming the artefact can be shared at all and
isn’t completely private/proprietary. This poses a new challenge; how do
we make sure the authors are reporting correct/real results without any
means for external validation? Perhaps we can’t, and trust is the only
choice.</p>
<p>All of this also puts to one side the lack of replication studies
done in our field. Despite many conferences having “replication of
previous results welcome” in their Call for Papers very few seem to
happen, and there is reticence among researchers to spend time doing
them for fear of time being “wasted”. ECOOP’s Call for Papers <a
href="https://2024.ecoop.org/track/ecoop-2024-papers#Call-for-Papers">this
year</a> specifically has a category for Replication:</p>
<blockquote>
<p><strong>Replication.</strong> An empirical evaluation that
reconstructs a published experiment in a different context in order to
validate the results of that earlier work.</p>
</blockquote>
<p>As it did <a
href="https://2023.ecoop.org/track/ecoop-2023-papers#Call-for-Papers">last
year</a>, although it seemed no papers were accepted in this category <a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.</p>
<h2 id="what-are-we-to-do"><span class="headline-word">What</span> <span
class="headline-word">are</span> <span class="headline-word">we</span>
<span class="headline-word">to</span> <span
class="headline-word">do?</span></h2>
<p>I don’t know. There are lots of challenges for replicating science,
some due to the complexity of experiments, some because of licensing and
protection of IP, and some simply down to access to unusual hardware.
The current approach to artefact evaluation gives coarse descriptions of
artefacts as they could be evaluated to the satisfaction of a small
committee with somewhat woolly requirements. The “grades” available
through the ACM badging do not tell the whole story, so some work there
would be helpful if nothing else.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>there was a “replication” track at ECOOP 2023, but it
was a different sort of replication.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  <footer>
    &copy; 2025, Simon Cooksey. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a><img src="/blog/icons/cc.svg" ><img src="/blog/icons/by.svg" ><img src="/blog/icons/nc.svg" ><img src="/blog/icons/sa.svg" >
  </footer>
  </main>
  <style></style>
</body>
</html>
